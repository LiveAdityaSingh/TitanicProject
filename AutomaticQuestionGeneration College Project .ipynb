{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPuv7IZxQLpv8AuUEZW2+ZX"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0RIb-AnDyQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#P3 Automatic Question Generation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asGI74GhFg4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Common imports\n",
        "import pandas as pd\n",
        "from IPython.display import Markdown, display, clear_output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGdcNsOfKggR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "490acf51-8385-4b86-fe74-5d66556a1448"
      },
      "source": [
        "import _pickle as cPickle\n",
        "from pathlib import Path\n",
        "\n",
        "def dumpPickle(fileName, content):\n",
        "    pickleFile = open(fileName, 'wb')\n",
        "    cPickle.dump(content, pickleFile, -1)\n",
        "    pickleFile.close()\n",
        "\n",
        "def loadPickle(fileName):    \n",
        "    file = open(fileName, 'rb')\n",
        "    content = cPickle.load(file)\n",
        "    file.close()\n",
        "    \n",
        "    return content\n",
        "    \n",
        "def pickleExists(fileName):\n",
        "    file = Path(fileName)\n",
        "    \n",
        "    if file.is_file():\n",
        "        return True\n",
        "    \n",
        "    return False\n",
        "\n",
        "print(\"Proceed to next step\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Proceed to next step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57tVrP4cKuFd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "76fc2273-2f25-4124-e51b-69baafee2fbd"
      },
      "source": [
        "#Data Exploration\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "#Extract answers and the sentence they are in\n",
        "def extractAnswers(qas, doc):\n",
        "    answers = []\n",
        "\n",
        "    senStart = 0\n",
        "    senId = 0\n",
        "\n",
        "    for sentence in doc.sents:\n",
        "        senLen = len(sentence.text)\n",
        "\n",
        "        for answer in qas:\n",
        "            answerStart = answer['answers'][0]['answer_start']\n",
        "\n",
        "            if (answerStart >= senStart and answerStart < (senStart + senLen)):\n",
        "                answers.append({'sentenceId': senId, 'text': answer['answers'][0]['text']})\n",
        "\n",
        "        senStart += senLen\n",
        "        senId += 1\n",
        "    \n",
        "    return answers\n",
        "\n",
        "#TODO - Clean answers from stopwords?\n",
        "def tokenIsAnswer(token, sentenceId, answers):\n",
        "    for i in range(len(answers)):\n",
        "        if (answers[i]['sentenceId'] == sentenceId):\n",
        "            if (answers[i]['text'] == token):\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "#Save named entities start points\n",
        "\n",
        "def getNEStartIndexs(doc):\n",
        "    neStarts = {}\n",
        "    for ne in doc.ents:\n",
        "        neStarts[ne.start] = ne\n",
        "        \n",
        "    return neStarts \n",
        "\n",
        "def getSentenceStartIndexes(doc):\n",
        "    senStarts = []\n",
        "    \n",
        "    for sentence in doc.sents:\n",
        "        senStarts.append(sentence[0].i)\n",
        "    \n",
        "    return senStarts\n",
        "    \n",
        "def getSentenceForWordPosition(wordPos, senStarts):\n",
        "    for i in range(1, len(senStarts)):\n",
        "        if (wordPos < senStarts[i]):\n",
        "            return i - 1\n",
        "#Raw text to array      \n",
        "def addWordsForParagrapgh(newWords, text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    neStarts = getNEStartIndexs(doc)\n",
        "    senStarts = getSentenceStartIndexes(doc)\n",
        "    \n",
        "    #index of word in spacy doc text\n",
        "    i = 0\n",
        "    \n",
        "    while (i < len(doc)):\n",
        "        #If the token is a start of a Named Entity, add it and push to index to end of the NE\n",
        "        if (i in neStarts):\n",
        "            word = neStarts[i]\n",
        "            #add word\n",
        "            currentSentence = getSentenceForWordPosition(word.start, senStarts)\n",
        "            wordLen = word.end - word.start\n",
        "            shape = ''\n",
        "            for wordIndex in range(word.start, word.end):\n",
        "                shape += (' ' + doc[wordIndex].shape_)\n",
        "\n",
        "            newWords.append([word.text,\n",
        "                            0,\n",
        "                            0,\n",
        "                            currentSentence,\n",
        "                            wordLen,\n",
        "                            word.label_,\n",
        "                            None,\n",
        "                            None,\n",
        "                            None,\n",
        "                            shape])\n",
        "            i = neStarts[i].end - 1\n",
        "        #If not a NE, add the word if it's not a stopword or a non-alpha (not regular letters)\n",
        "        else:\n",
        "            if (doc[i].is_stop == False and doc[i].is_alpha == True):\n",
        "                word = doc[i]\n",
        "\n",
        "                currentSentence = getSentenceForWordPosition(i, senStarts)\n",
        "                wordLen = 1\n",
        "\n",
        "                newWords.append([word.text,\n",
        "                                0,\n",
        "                                0,\n",
        "                                currentSentence,\n",
        "                                wordLen,\n",
        "                                None,\n",
        "                                word.pos_,\n",
        "                                word.tag_,\n",
        "                                word.dep_,\n",
        "                                word.shape_])\n",
        "        i += 1\n",
        "\n",
        "def oneHotEncodeColumns(df):\n",
        "    columnsToEncode = ['NER', 'POS', \"TAG\", 'DEP']\n",
        "\n",
        "    for column in columnsToEncode:\n",
        "        one_hot = pd.get_dummies(df[column])\n",
        "        one_hot = one_hot.add_prefix(column + '_')\n",
        "\n",
        "        df = df.drop(column, axis = 1)\n",
        "        df = df.join(one_hot)\n",
        "    \n",
        "    return df\n",
        "\n",
        "print(\"proceed to next step\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "proceed to next step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKR2C0pWKyJ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "56c659ee-3e15-4918-ade3-2703e1512d28"
      },
      "source": [
        "def generateDf(text):\n",
        "    words = []\n",
        "    addWordsForParagrapgh(words, text)\n",
        "\n",
        "    wordColums = ['text', 'titleId', 'paragrapghId', 'sentenceId','wordCount', 'NER', 'POS', 'TAG', 'DEP','shape']\n",
        "    df = pd.DataFrame(words, columns=wordColums)\n",
        "    \n",
        "    return df\n",
        "print(\"proceed to next step\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "proceed to next step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlAa0AdIKzUM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "61489a91-2002-435a-f668-3af71026f7a0"
      },
      "source": [
        "def prepareDf(df):\n",
        "    #One-hot encoding\n",
        "    wordsDf = oneHotEncodeColumns(df)\n",
        "\n",
        "    #Drop un-usefull columns\n",
        "    columnsToDrop = ['text', 'titleId', 'paragrapghId', 'sentenceId', 'shape']\n",
        "    wordsDf = wordsDf.drop(columnsToDrop, axis = 1)\n",
        "\n",
        "    #Add token colums \n",
        "    predictorColumns = ['wordCount','NER_CARDINAL','NER_DATE','NER_EVENT','NER_FAC','NER_GPE','NER_LANGUAGE','NER_LAW','NER_LOC','NER_MONEY','NER_NORP','NER_ORDINAL','NER_ORG','NER_PERCENT','NER_PERSON','NER_PRODUCT','NER_QUANTITY','NER_TIME','NER_WORK_OF_ART','POS_ADJ','POS_ADP','POS_ADV','POS_CCONJ','POS_DET','POS_INTJ','POS_NOUN','POS_NUM','POS_PART','POS_PRON','POS_PROPN','POS_PUNCT','POS_SYM','POS_VERB','POS_X','TAG_''','TAG_-LRB-','TAG_.','TAG_ADD','TAG_AFX','TAG_CC','TAG_CD','TAG_DT','TAG_EX','TAG_FW','TAG_IN','TAG_JJ','TAG_JJR','TAG_JJS','TAG_LS','TAG_MD','TAG_NFP','TAG_NN','TAG_NNP','TAG_NNPS','TAG_NNS','TAG_PDT','TAG_POS','TAG_PRP','TAG_PRP$','TAG_RB','TAG_RBR','TAG_RBS','TAG_RP','TAG_SYM','TAG_TO','TAG_UH','TAG_VB','TAG_VBD','TAG_VBG','TAG_VBN','TAG_VBP','TAG_VBZ','TAG_WDT','TAG_WP','TAG_WRB','TAG_XX','DEP_ROOT','DEP_acl','DEP_acomp','DEP_advcl','DEP_advmod','DEP_agent','DEP_amod','DEP_appos','DEP_attr','DEP_aux','DEP_auxpass','DEP_case','DEP_cc','DEP_ccomp','DEP_compound','DEP_conj','DEP_csubj','DEP_csubjpass','DEP_dative','DEP_dep','DEP_det','DEP_dobj','DEP_expl','DEP_intj','DEP_mark','DEP_meta','DEP_neg','DEP_nmod','DEP_npadvmod','DEP_nsubj','DEP_nsubjpass','DEP_nummod','DEP_oprd','DEP_parataxis','DEP_pcomp','DEP_pobj','DEP_poss','DEP_preconj','DEP_predet','DEP_prep','DEP_prt','DEP_punct','DEP_quantmod','DEP_relcl','DEP_xcomp']\n",
        "\n",
        "    for feature in predictorColumns:\n",
        "        if feature not in wordsDf.columns:\n",
        "            wordsDf[feature] = 0\n",
        "    \n",
        "    return wordsDf\n",
        "print(\"proceed to next step\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "proceed to next step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZKxaaA8K3ZS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d99a77d-9638-4d4f-92fb-da5bc769c167"
      },
      "source": [
        "def predictWords(wordsDf, df):\n",
        "    \n",
        "    predictorPickleName = 'data/pickles/nb-predictor.pkl'\n",
        "    predictor = loadPickle(predictorPickleName)\n",
        "    \n",
        "    y_pred = predictor.predict_proba(wordsDf)\n",
        "\n",
        "    labeledAnswers = []\n",
        "    for i in range(len(y_pred)):\n",
        "        labeledAnswers.append({'word': df.iloc[i]['text'], 'prob': y_pred[i][0]})\n",
        "    \n",
        "    return labeledAnswers\n",
        "\n",
        "print(\"proceed to next step\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "proceed to next step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGlf26QjK6yL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dcc28743-3a4b-476e-8519-59c98c2a8574"
      },
      "source": [
        "def blankAnswer(firstTokenIndex, lastTokenIndex, sentStart, sentEnd, doc):\n",
        "    leftPartStart = doc[sentStart].idx\n",
        "    leftPartEnd = doc[firstTokenIndex].idx\n",
        "    rightPartStart = doc[lastTokenIndex].idx + len(doc[lastTokenIndex])\n",
        "    rightPartEnd = doc[sentEnd - 1].idx + len(doc[sentEnd - 1])\n",
        "    \n",
        "    question = doc.text[leftPartStart:leftPartEnd] + '_____' + doc.text[rightPartStart:rightPartEnd]\n",
        "    \n",
        "    return question\n",
        "\n",
        "print(\"proceed to next step\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "proceed to next step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5-eaw58K9or",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6524bd2c-5551-4f91-f1c7-42e0a7475926"
      },
      "source": [
        "#token to sentences\n",
        "\n",
        "def addQuestions(answers, text):\n",
        "    doc = nlp(text)\n",
        "    currAnswerIndex = 0\n",
        "    qaPair = []\n",
        "\n",
        "    #Check wheter each token is the next answer\n",
        "    for sent in doc.sents:\n",
        "        for token in sent:\n",
        "            \n",
        "            #If all the answers have been found, stop looking\n",
        "            if currAnswerIndex >= len(answers):\n",
        "                break\n",
        "            \n",
        "            #In the case where the answer is consisted of more than one token, check the following tokens as well.\n",
        "            answerDoc = nlp(answers[currAnswerIndex]['word'])\n",
        "            answerIsFound = True\n",
        "            \n",
        "            for j in range(len(answerDoc)):\n",
        "                if token.i + j >= len(doc) or doc[token.i + j].text != answerDoc[j].text:\n",
        "                    answerIsFound = False\n",
        "           \n",
        "            #If the current token is corresponding with the answer, add it \n",
        "            if answerIsFound:\n",
        "                question = blankAnswer(token.i, token.i + len(answerDoc) - 1, sent.start, sent.end, doc)\n",
        "                \n",
        "                qaPair.append({'question' : question, 'answer': answers[currAnswerIndex]['word'], 'prob': answers[currAnswerIndex]['prob']})\n",
        "                \n",
        "                currAnswerIndex += 1\n",
        "                \n",
        "    return qaPair\n",
        "\n",
        "print(\"proceed to next step\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "proceed to next step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14H5p-G4LAyF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "97aee466-8367-4499-e53e-f589ed565374"
      },
      "source": [
        "#Follow-up\n",
        "def sortAnswers(qaPairs):\n",
        "    orderedQaPairs = sorted(qaPairs, key=lambda qaPair: qaPair['prob'])\n",
        "    \n",
        "    return orderedQaPairs\n",
        "\n",
        "print(\"proceed to next step\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "proceed to next step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhhIpNG8LGP3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "3f9f1d8d-1205-492d-e0e0-77efdc7b766b"
      },
      "source": [
        "print(\"Wrong answers in progress\")\n",
        "import gensim\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "glove_file = 'data/embeddings/glove.6B.300d.txt'\n",
        "tmp_file = 'data/embeddings/word2vec-glove.6B.300d.txt'\n",
        "\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "glove2word2vec(glove_file, tmp_file)\n",
        "model = KeyedVectors.load_word2vec_format(tmp_file)\n",
        "\n",
        "\n",
        "def generate_distractors(answer, count):\n",
        "    answer = str.lower(answer)\n",
        "    try:\n",
        "        closestWords = model.most_similar(positive=[answer], topn=count)\n",
        "    except:\n",
        "        return []\n",
        "    distractors = list(map(lambda x: x[0], closestWords))[0:count]\n",
        "    \n",
        "    return distractors\n",
        "\n",
        "def addDistractors(qaPairs, count):\n",
        "    for qaPair in qaPairs:\n",
        "        distractors = generate_distractors(qaPair['answer'], count)\n",
        "        qaPair['distractors'] = distractors\n",
        "    \n",
        "    return qaPairs\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wrong answers in progress\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-105-34a9b4c7d6e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscripts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglove2word2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglove2word2vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mglove2word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1436\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid vector on line %s (is this really the text format?)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mline_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m                 \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid vector on line 402 (is this really the text format?)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nI5oX6RLNIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Wrap-up and display\n",
        "def generateQuestions(text, count):\n",
        "    \n",
        "    # Extract words \n",
        "    df = generateDf(text)\n",
        "    wordsDf = prepareDf(df)\n",
        "    \n",
        "    # Predict \n",
        "    labeledAnswers = predictWords(wordsDf, df)\n",
        "    \n",
        "    # Transform questions\n",
        "    qaPairs = addQuestions(labeledAnswers, text)\n",
        "    \n",
        "    # Pick the best questions\n",
        "    orderedQaPairs = sortAnswers(qaPairs)\n",
        "    \n",
        "    # Generate distractors\n",
        "    questions = addDistractors(orderedQaPairs[:count], 4)\n",
        "    \n",
        "    # Print\n",
        "    for i in range(count):\n",
        "        display(Markdown('### Question ' + str(i + 1) + ':'))\n",
        "        print(questions[i]['question'])\n",
        "\n",
        "        display(Markdown('#### Answer:'))\n",
        "        print(questions[i]['answer'])\n",
        "        \n",
        "        display(Markdown('#### Incorrect answers:'))\n",
        "        for distractor in questions[i]['distractors']:\n",
        "            print(distractor)\n",
        "        \n",
        "        print()\n",
        "\n",
        "print(\"Display Ready\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q06A1Y2LQaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "with open (\"data.txt\", \"r\") as myfile:\n",
        "                text=myfile.read().replace('\\n',' ')\n",
        "print(\"Test Data: %s\",text)\n",
        "generateQuestions(text, 10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}